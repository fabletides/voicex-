#!/usr/bin/env python3
"""
VoiceX ML Training Pipeline –¥–ª—è Vibravox Dataset
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ—Å–∞ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ VoiceX
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'sample_rate': 22050,
    'n_fft': 2048,
    'hop_length': 512,
    'n_mels': 80,
    'duration': 3.0,  # —Å–µ–∫—É–Ω–¥—ã
    'batch_size': 32,
    'epochs': 100,
    'learning_rate': 0.001,
    'model_size': 'small',  # small, medium, large
}

class VibravoxDataProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö Vibravox –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π VoiceX"""
    
    def __init__(self, data_path, config=CONFIG):
        self.data_path = Path(data_path)
        self.config = config
        self.scaler = StandardScaler()
        
    def load_vibravox_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö Vibravox"""
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Vibravox...")
        
        # –ü–æ–∏—Å–∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
        audio_files = []
        metadata = []
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ Vibravox: speech/, throat_microphone/, etc.
        speech_dir = self.data_path / "speech"
        throat_dir = self.data_path / "throat_microphone"
        
        if not speech_dir.exists() or not throat_dir.exists():
            raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ speech/ –∏ throat_microphone/ –≤ {self.data_path}")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
        for speech_file in speech_dir.glob("**/*.wav"):
            # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –≥–æ—Ä–ª–æ–≤–æ–≥–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
            relative_path = speech_file.relative_to(speech_dir)
            throat_file = throat_dir / relative_path
            
            if throat_file.exists():
                audio_files.append({
                    'speech_path': str(speech_file),
                    'throat_path': str(throat_file),
                    'speaker_id': self._extract_speaker_id(speech_file),
                    'session_id': self._extract_session_id(speech_file),
                    'utterance_id': speech_file.stem
                })
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(audio_files)} –ø–∞—Ä –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤")
        return audio_files
    
    def _extract_speaker_id(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏–∑ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞"""
        parts = str(file_path).split('/')
        for part in parts:
            if part.startswith('speaker_') or part.startswith('spk'):
                return part
        return "unknown"
    
    def _extract_session_id(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID —Å–µ—Å—Å–∏–∏ –∏–∑ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞"""
        parts = str(file_path).split('/')
        for part in parts:
            if part.startswith('session_') or part.startswith('sess'):
                return part
        return "unknown"
    
    def extract_audio_features(self, audio_path, is_throat=False):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
            y, sr = librosa.load(audio_path, sr=self.config['sample_rate'])
            
            # –û–±—Ä–µ–∑–∫–∞/–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            target_length = int(self.config['sample_rate'] * self.config['duration'])
            if len(y) > target_length:
                y = y[:target_length]
            else:
                y = np.pad(y, (0, target_length - len(y)), mode='constant')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = {}
            
            # Mel-spectrogram
            mel_spec = librosa.feature.melspectrogram(
                y=y, 
                sr=sr,
                n_fft=self.config['n_fft'],
                hop_length=self.config['hop_length'],
                n_mels=self.config['n_mels']
            )
            features['mel_spectrogram'] = librosa.power_to_db(mel_spec, ref=np.max)
            
            # MFCC
            mfcc = librosa.feature.mfcc(
                y=y, 
                sr=sr, 
                n_mfcc=13,
                n_fft=self.config['n_fft'],
                hop_length=self.config['hop_length']
            )
            features['mfcc'] = mfcc
            
            # Pitch (F0)
            f0, voiced_flag, voiced_probs = librosa.pyin(
                y, 
                fmin=librosa.note_to_hz('C2'), 
                fmax=librosa.note_to_hz('C7'),
                sr=sr
            )
            features['f0'] = np.nan_to_num(f0)
            
            # Spectral features
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
            zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
            
            features['spectral_centroid'] = spectral_centroids
            features['spectral_rolloff'] = spectral_rolloff
            features['zcr'] = zero_crossing_rate
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≥–æ—Ä–ª–æ–≤–æ–≥–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
            if is_throat:
                # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è throat microphone
                chroma = librosa.feature.chroma_stft(y=y, sr=sr)
                features['chroma'] = chroma
                
                # –≠–Ω–µ—Ä–≥–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –ø–æ–ª–æ—Å–∞—Ö
                features['energy'] = librosa.feature.rms(y=y)[0]
            
            return features
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {audio_path}: {e}")
            return None
    
    def create_training_dataset(self, audio_files, test_size=0.2):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        X_speech = []  # –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—ã—á–Ω–æ–π —Ä–µ—á–∏
        X_throat = []  # –ü—Ä–∏–∑–Ω–∞–∫–∏ –≥–æ—Ä–ª–æ–≤–æ–≥–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
        y_speech = []  # –¶–µ–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–æ–±—ã—á–Ω–∞—è —Ä–µ—á—å)
        
        valid_samples = 0
        
        for file_info in tqdm(audio_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤"):
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω–æ–π —Ä–µ—á–∏ (target)
            speech_features = self.extract_audio_features(file_info['speech_path'], is_throat=False)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ—Ä–ª–æ–≤–æ–≥–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (input)
            throat_features = self.extract_audio_features(file_info['throat_path'], is_throat=True)
            
            if speech_features is not None and throat_features is not None:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º mel-spectrogram –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                X_throat.append(throat_features['mel_spectrogram'].flatten())
                y_speech.append(speech_features['mel_spectrogram'].flatten())
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                additional_throat = np.concatenate([
                    throat_features['mfcc'].flatten(),
                    throat_features['f0'][:len(throat_features['spectral_centroid'])],
                    throat_features['spectral_centroid'],
                    throat_features['zcr']
                ])
                
                additional_speech = np.concatenate([
                    speech_features['mfcc'].flatten(),
                    speech_features['f0'][:len(speech_features['spectral_centroid'])],
                    speech_features['spectral_centroid'],
                    speech_features['zcr']
                ])
                
                X_speech.append(additional_throat)
                valid_samples += 1
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {valid_samples} –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy arrays
        X_throat = np.array(X_throat)
        y_speech = np.array(y_speech)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        X_throat_scaled = self.scaler.fit_transform(X_throat)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X_throat_scaled, y_speech, 
            test_size=test_size, 
            random_state=42,
            stratify=None
        )
        
        print(f"üìä –†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞:")
        print(f"   Train: X={X_train.shape}, y={y_train.shape}")
        print(f"   Test:  X={X_test.shape}, y={y_test.shape}")
        
        return (X_train, X_test, y_train, y_test)

class VoiceXModel:
    """–ú–æ–¥–µ–ª—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ—Å–∞ –¥–ª—è VoiceX"""
    
    def __init__(self, input_dim, output_dim, model_size='small'):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.model_size = model_size
        self.model = None
        
    def build_model(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        print(f"üîÑ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–∞ '{self.model_size}'...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
        if self.model_size == 'small':
            hidden_layers = [256, 128, 64]
            dropout_rate = 0.3
        elif self.model_size == 'medium':
            hidden_layers = [512, 256, 128, 64]
            dropout_rate = 0.4
        else:  # large
            hidden_layers = [1024, 512, 256, 128, 64]
            dropout_rate = 0.5
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏
        model = keras.Sequential([
            keras.layers.Input(shape=(self.input_dim,)),
            keras.layers.BatchNormalization(),
        ])
        
        # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
        for i, units in enumerate(hidden_layers):
            model.add(keras.layers.Dense(units, activation='relu'))
            model.add(keras.layers.BatchNormalization())
            model.add(keras.layers.Dropout(dropout_rate))
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        model.add(keras.layers.Dense(self.output_dim, activation='linear'))
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),
            loss='mse',
            metrics=['mae']
        )
        
        self.model = model
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞. –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}")
        return model
    
    def train(self, X_train, y_train, X_test, y_test, epochs=100):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        # Callbacks
        callbacks = [
            keras.callbacks.EarlyStopping(
                patience=15, 
                restore_best_weights=True,
                monitor='val_loss'
            ),
            keras.callbacks.ReduceLROnPlateau(
                factor=0.5, 
                patience=10,
                monitor='val_loss'
            ),
            keras.callbacks.ModelCheckpoint(
                'best_voicex_model.h5',
                save_best_only=True,
                monitor='val_loss'
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        history = self.model.fit(
            X_train, y_train,
            batch_size=CONFIG['batch_size'],
            epochs=epochs,
            validation_data=(X_test, y_test),
            callbacks=callbacks,
            verbose=1
        )
        
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return history
    
    def convert_to_tflite(self, output_path='voicex_model.tflite'):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ TensorFlow Lite –¥–ª—è ESP32"""
        print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ TensorFlow Lite...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –º–∏–∫—Ä–æ–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–≤
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float32]
        
        # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
        converter.representative_dataset = self._representative_dataset
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
        tflite_model = converter.convert()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path} ({len(tflite_model)/1024:.1f} KB)")
        return tflite_model
    
    def _representative_dataset(self):
        """–†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏"""
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        for _ in range(100):
            yield [np.random.float32(np.random.random((1, self.input_dim)))]

def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Loss
    ax1.plot(history.history['loss'], label='Training Loss')
    ax1.plot(history.history['val_loss'], label='Validation Loss')
    ax1.set_title('Model Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    
    # MAE
    ax2.plot(history.history['mae'], label='Training MAE')
    ax2.plot(history.history['val_mae'], label='Validation MAE')
    ax2.set_title('Model MAE')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('MAE')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def evaluate_model(model, X_test, y_test, scaler):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    print("üîÑ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = model.predict(X_test)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    mse = np.mean((y_test - y_pred) ** 2)
    mae = np.mean(np.abs(y_test - y_pred))
    
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏:")
    print(f"   MSE: {mse:.6f}")
    print(f"   MAE: {mae:.6f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plt.figure(figsize=(10, 6))
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    sample_idx = np.random.choice(len(y_test), 5)
    
    for i, idx in enumerate(sample_idx):
        plt.subplot(2, 3, i+1)
        plt.plot(y_test[idx][:100], label='True', alpha=0.7)
        plt.plot(y_pred[idx][:100], label='Predicted', alpha=0.7)
        plt.title(f'Sample {idx}')
        plt.legend()
    
    plt.tight_layout()
    plt.savefig('model_predictions.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return {'mse': mse, 'mae': mae}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    parser = argparse.ArgumentParser(description='VoiceX ML Training Pipeline')
    parser.add_argument('--data_path', type=str, required=True, 
                       help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É Vibravox')
    parser.add_argument('--model_size', type=str, default='small',
                       choices=['small', 'medium', 'large'],
                       help='–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏')
    parser.add_argument('--epochs', type=int, default=100,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--output_dir', type=str, default='./models',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("üöÄ –ó–∞–ø—É—Å–∫ VoiceX ML Training Pipeline")
    print("=" * 50)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    processor = VibravoxDataProcessor(args.data_path, CONFIG)
    audio_files = processor.load_vibravox_data()
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    X_train, X_test, y_train, y_test = processor.create_training_dataset(audio_files)
    
    # 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_builder = VoiceXModel(
        input_dim=X_train.shape[1],
        output_dim=y_train.shape[1],
        model_size=args.model_size
    )
    model = model_builder.build_model()
    
    # 4. –û–±—É—á–µ–Ω–∏–µ
    history = model_builder.train(X_train, y_train, X_test, y_test, epochs=args.epochs)
    
    # 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plot_training_history(history)
    
    # 6. –û—Ü–µ–Ω–∫–∞
    metrics = evaluate_model(model, X_test, y_test, processor.scaler)
    
    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    # Keras –º–æ–¥–µ–ª—å
    keras_path = os.path.join(args.output_dir, 'voicex_model.h5')
    model.save(keras_path)
    print(f"‚úÖ Keras –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {keras_path}")
    
    # TensorFlow Lite –º–æ–¥–µ–ª—å
    tflite_path = os.path.join(args.output_dir, 'voicex_model.tflite')
    model_builder.convert_to_tflite(tflite_path)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞
    import joblib
    scaler_path = os.path.join(args.output_dir, 'scaler.pkl')
    joblib.dump(processor.scaler, scaler_path)
    print(f"‚úÖ –°–∫–µ–π–ª–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {scaler_path}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config_path = os.path.join(args.output_dir, 'config.json')
    with open(config_path, 'w') as f:
        json.dump({
            'CONFIG': CONFIG,
            'model_size': args.model_size,
            'input_dim': X_train.shape[1],
            'output_dim': y_train.shape[1],
            'metrics': metrics
        }, f, indent=2)
    
    print("=" * 50)
    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    print(f"üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output_dir}")
    print(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: MSE={metrics['mse']:.6f}, MAE={metrics['mae']:.6f}")

if __name__ == "__main__":
    main()